{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## Introduction to Regularization\n",
    "\n",
    "While **linear regression** works well in many cases, it can suffer from **overfitting** when the model becomes too complex or the dataset contains noise. Overfitting occurs when a model captures not only the true underlying pattern but also random fluctuations or noise in the data. This leads to poor generalization on unseen data.\n",
    "\n",
    "One way to combat overfitting is to apply **regularization**, which modifies the loss function by adding a penalty for large model parameters. The idea is to constrain the model's flexibility and encourage simpler models that generalize better.\n",
    "\n",
    "In this chapter, we will introduce two commonly used regularization techniques:\n",
    "\n",
    "- **Ridge Regression** (also known as L2 regularization)\n",
    "- **LASSO Regression** (also known as L1 regularization)\n",
    "\n",
    "## Ridge Regression (L2 Regularization)\n",
    "\n",
    "In **ridge regression**, we modify the ordinary least squares loss function by adding a penalty proportional to the square of the parameters' magnitudes. This penalty discourages large parameters, leading to a smoother model that is less likely to overfit.\n",
    "\n",
    "The objective function for ridge regression is:\n",
    "\n",
    "```{math}\n",
    "\\mathcal{l}_{ridge} = \\sum_i \\left( f(\\mathbf{x}_i) - y_i \\right)^2 + \\lambda \\sum_j \\beta_j^2\n",
    "```\n",
    "\n",
    "```{include} ../_includes/honey1.html\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $\\lambda$ is a **regularization parameter** that controls the strength of the penalty. When $\\lambda = 0$, ridge regression reduces to ordinary least squares. As $\\lambda$ increases, the penalty becomes stronger.\n",
    "- $\\beta_j$ are the model parameters (excluding the intercept).\n",
    "\n",
    "The key idea here is that by penalizing the size of the parameters, we shrink them toward zero, which can help mitigate overfitting.\n",
    "\n",
    "### Ridge Regression Using Scikit-learn\n",
    "\n",
    "The **scikit-learn** package provides a `Ridge` class that implements ridge regression. Below is an example of how to apply ridge regression to a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "rng = np.random.RandomState(1)\n",
    "N=20\n",
    "x = rng.rand(N)\n",
    "y_non_linear = np.sin(10.*x) + 0.1 * rng.randn(N)\n",
    "y_non_linear = 4 * x**2 - 10 * x**4  + 6 * x**6 + 0.1 * rng.randn(N)\n",
    "\n",
    "# Define the polynomial model function\n",
    "def polynomial_model(x, params, degree=7):\n",
    "    x_poly = np.vstack([x**i for i in range(degree + 1)]).T\n",
    "    return np.dot(x_poly, params)\n",
    "\n",
    "def minimize_loss(loss, model, x, y, num_params):\n",
    "    initial_params = np.random.randn(num_params)\n",
    "    return minimize(loss, initial_params, args=(model, x, y))\n",
    "\n",
    "def rss_loss(params, model_function, X, y):\n",
    "    predictions = model_function(x, params)  # Predicted values\n",
    "    residual_sum_squares = np.sum((y - predictions) ** 2)  # RSS\n",
    "    return residual_sum_squares\n",
    "\n",
    "# Define ridge loss function\n",
    "def ridge_loss(params, model_function, X, y, alpha=0.1):\n",
    "    residual_sum_squares =  rss_loss(params, model_function, X, y)  # RSS\n",
    "    l2_penalty = alpha * np.sum(params**2)\n",
    "    return residual_sum_squares + l2_penalty\n",
    "\n",
    "# Optimize the parameters of our model using the ridge loss function\n",
    "result_poly_rss = minimize_loss(rss_loss, polynomial_model, x, y_non_linear,8)\n",
    "result_poly_ridge = minimize_loss(ridge_loss, polynomial_model, x, y_non_linear,8)\n",
    "\n",
    "poly_params_rss = result_poly_rss.x\n",
    "poly_params_ridge = result_poly_ridge.x\n",
    "\n",
    "# Generate fit lines\n",
    "xfit = np.linspace(np.min(x), np.max(x), 1000)\n",
    "yfit_poly_rss = polynomial_model(xfit, poly_params_rss)\n",
    "yfit_poly_ridge = polynomial_model(xfit, poly_params_ridge)\n",
    "\n",
    "sns.scatterplot(x=x, y=y_non_linear, label=\"Data\")\n",
    "plt.plot(xfit, yfit_poly_rss, color='r',label=\"RSS\")\n",
    "plt.plot(xfit, yfit_poly_ridge, color='b',label=\"Ridge\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Polynomial parameters (RSS):\", poly_params_rss)\n",
    "print(\"Polynomial parameters (Ridge):\", poly_params_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "In this example, `alpha` corresponds to $\\lambda$ and controls the regularization strength. By tuning this parameter, we can adjust the model's complexity.\n",
    "\n",
    "\n",
    "## LASSO Regression (L1 Regularization)\n",
    "\n",
    "**LASSO regression** (Least Absolute Shrinkage and Selection Operator) is another regularization technique. Instead of penalizing the sum of squared parameters, LASSO penalizes the sum of the absolute values of the parameters. This results in sparse solutions, where some of the parameters may become exactly zero.\n",
    "\n",
    "The objective function for LASSO regression is:\n",
    "\n",
    "```{math}\n",
    "\\mathcal{l}_{lasso} = \\sum_i \\left( f(\\mathbf{x}_i) - y_i \\right)^2 + \\lambda \\sum_j |\\beta_j|\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $\\lambda$ again controls the regularization strength.\n",
    "- The absolute value penalty leads to **sparse models**, meaning that LASSO can perform **feature selection** by setting some parameters to zero.\n",
    "\n",
    "LASSO is particularly useful when we have many features, as it can identify and retain only the most important ones.\n",
    "\n",
    "### LASSO Regression Using Scikit-learn\n",
    "\n",
    "The **scikit-learn** package provides a `Lasso` class for L1-regularized regression. Below is an example of LASSO applied to a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define LASSO loss function\n",
    "def lasso_loss(params, model_function, X, y, alpha=0.1):\n",
    "    residual_sum_squares =  rss_loss(params, model_function, X, y)  # RSS\n",
    "    l1_penalty = alpha * np.sum(np.abs(params))  # L1 regularization\n",
    "    return residual_sum_squares + l1_penalty\n",
    "\n",
    "# Optimize the parameters of our model using the lasso loss function\n",
    "result_poly_lasso = minimize_loss(lasso_loss, polynomial_model, x, y_non_linear,8)\n",
    "\n",
    "poly_params_lasso = result_poly_lasso.x\n",
    "\n",
    "# Generate fit lines\n",
    "yfit_poly_lasso = polynomial_model(xfit, poly_params_lasso)\n",
    "\n",
    "sns.scatterplot(x=x, y=y_non_linear, label=\"Data\")\n",
    "plt.plot(xfit, yfit_poly_rss, color='r',label=\"RSS\")\n",
    "plt.plot(xfit, yfit_poly_lasso, color='b',label=\"LASSO\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print parameters\n",
    "print(\"Polynomial parameters (RSS):\", poly_params_rss)\n",
    "print(\"Polynomial parameters (LASSO):\", poly_params_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "As with ridge regression, `alpha` controls the strength of regularization. When $\\alpha$ is large, more parameters will be set to values close to zero, resulting in a simpler model. It is worth noting that there are better optimization schemes available than regular gradient decent optimizers specially [targeting LASSO regression](https://en.wikipedia.org/wiki/Least-angle_regression), which helps keeping parameters at exactly 0 (not just close to 0).\n",
    "\n",
    "## Comparing Ridge and LASSO\n",
    "\n",
    "While both **ridge** and **LASSO** regression aim to reduce overfitting by penalizing large parameters, they behave differently:\n",
    "\n",
    "- **Ridge regression** tends to shrink parameters but rarely sets them exactly to zero. This means that it keeps all features in the model, but the influence of less important features is reduced.\n",
    "- **LASSO regression**, on the other hand, can shrink parameters all the way to zero, effectively performing feature selection.\n",
    "\n",
    "The choice between ridge and LASSO depends on the problem:\n",
    "- If you believe that all features are relevant and you want to shrink their influence, **ridge** might be the better option.\n",
    "- If you suspect that only a few features are truly important, **LASSO** can help by selecting those features automatically.\n",
    "\n",
    "## Elastic Net: Combining Ridge and LASSO\n",
    "\n",
    "In some cases, it can be beneficial to combine the strengths of ridge and LASSO regression. This is achieved with **Elastic Net**, which includes both L1 and L2 penalties.\n",
    "\n",
    "The objective function for Elastic Net is:\n",
    "\n",
    "```{math}\n",
    "\\mathcal{l}_{elasticnet} = \\sum_i \\left( f(\\mathbf{x}_i) - y_i \\right)^2 + \\lambda_1 \\sum_j |\\beta_j| + \\lambda_2 \\sum_j \\beta_j^2\n",
    "```\n",
    "\n",
    "Here, both $\\lambda_1$ and $\\lambda_2$ control the balance between L1 and L2 regularization. This method is particularly useful when there are correlations between features, as it can encourage a grouping effect, where correlated features tend to have similar parameters.\n",
    "\n",
    "### Elastic Net Using Scikit-learn\n",
    "\n",
    "The **scikit-learn** package also provides an `ElasticNet` class, which combines ridge and LASSO regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Fit Elastic Net model\n",
    "x_poly = np.vstack([x**i for i in range(8)]).T\n",
    "elastic_net_model = ElasticNet(alpha=0.005, l1_ratio=0.5)  # l1_ratio controls the balance between L1 and L2 regularization\n",
    "elastic_net_model.fit(x_poly, y_non_linear)\n",
    "\n",
    "# Predict and plot\n",
    "x_poly_fit = np.vstack([xfit**i for i in range(8)]).T\n",
    "yfit_elastic_net = elastic_net_model.predict(x_poly_fit)\n",
    "\n",
    "sns.scatterplot(x=x, y=y_non_linear, label=\"data\")\n",
    "plt.plot(xfit, yfit_elastic_net, color='m', label=\"elastic net\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "By tuning both the `alpha` and `l1_ratio` parameters, we can control the balance between ridge and LASSO regularization.\n",
    "\n",
    "\n",
    "## Why Does This Work?\n",
    "\n",
    "Regularization works by reducing the likelihood of overfitting through the addition of penalties for large model parameters. In regression models, parameters represent the strength and direction of the relationship between each feature and the target variable. When these parameters grow large, it can indicate that the model is reacting too strongly to specific details or noise in the training data, which may not hold for new data. This is a sign of overfitting: the model has not only learned the true underlying patterns but has also fit itself to random fluctuations or specific data instances in the training set.\n",
    "\n",
    "By adding penalties (as in L1 and L2 regularization), we shrink the size of the parameters. Smaller parameters prevent the model from placing too much importance on any one feature unless it truly adds significant predictive value. In L2 regularization (Ridge), for example, parameters are constrained by a penalty on their squared magnitudes, which promotes a smoother fit and reduces the model’s flexibility. L1 regularization (Lasso), on the other hand, may drive some parameters all the way to zero, effectively performing feature selection and allowing the model to focus only on the most important predictors.\n",
    "\n",
    "This constraint on parameter size simplifies the model, reducing its tendency to memorize the training data and helping it generalize better to unseen data. By focusing on essential relationships, regularization leads the model to capture the overall trend or signal in the data, rather than noise, enhancing its robustness and predictive power across different datasets.\n",
    "\n",
    "Here is a short section you can insert into your book text on why we refer to **L1** and **L2** regularization.\n",
    "\n",
    "### Why “L1” and “L2” Regularization?\n",
    "\n",
    "The labels **“L1”** and **“L2”** come from the notation of vector norms in mathematics. A [*norm*](https://en.wikipedia.org/wiki/Norm_(mathematics)) on a vector space is a function that assigns a non-negative length or size to each vector and satisfies certain axioms (positivity, homogeneity, triangle inequality). The **L1 norm** (also written $\\ell_1$-norm) of a vector $w = (w_1, w_2, \\dots, w_p)$ is\n",
    "\n",
    "```{math}\n",
    "||w||_1 = \\sum*{i=1}^p |w_i|.\n",
    "```\n",
    "\n",
    "The **L2 norm** (or (\\ell_2)-norm) is\n",
    "\n",
    "```{math}\n",
    "||w||_2 = \\sqrt{ \\sum*{i=1}^p w_i^2 },\n",
    "```\n",
    "\n",
    "which corresponds geometrically to the Euclidean length of the vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "substitutions": {
   "grunt": "granular"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
