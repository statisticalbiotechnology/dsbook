{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwlylH9QdZIo"
   },
   "source": [
    "# VAE analysis of two lung cancer sets\n",
    "\n",
    "Here we training a VAE of two different datasets within the TCGA. We will first merge the two datasets and subsequently try to separate the samples based on their latent variables. This is made in analogue with the notebook on PCA.\n",
    "\n",
    "First we retrieve our two TCGA lungcancer data from cbioportal.org. One of the sets are from [Lung Adenocarcinomas](https://en.wikipedia.org/wiki/Adenocarcinoma_of_the_lung) and the other is from [Lung Squamous Cell Carcinomas](https://en.wikipedia.org/wiki/Squamous-cell_carcinoma_of_the_lung). We first load our dataset,  however, I have hiddden the code fort this, as you have seen rthat in previous examples\n",
    "\n",
    "Particularly, we added code that is hidden in the other version of the notebook,that is not important for the understanding of the analysis, but can be found in the module tcga_read. Execute the code and proceed to next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    ![ ! -f \"dsbook/README.md\" ] && git clone https://github.com/statisticalbiotechnology/dsbook.git\n",
    "    my_path = \"dsbook/dsbook/common/\"\n",
    "else:\n",
    "    my_path = \"../common/\"\n",
    "sys.path.append(my_path) # Read local modules for tcga access and qvalue calculations\n",
    "import load_tcga as tcga\n",
    "\n",
    "luad = tcga.get_expression_data(my_path + \"../data/luad_tcga_pan_can_atlas_2018.tar.gz\", 'https://cbioportal-datahub.s3.amazonaws.com/luad_tcga_pan_can_atlas_2018.tar.gz',\"data_mrna_seq_v2_rsem.txt\")\n",
    "lusc = tcga.get_expression_data(my_path + \"../data/lusc_tcga_pan_can_atlas_2018.tar.gz\", 'https://cbioportal-datahub.s3.amazonaws.com/lusc_tcga_pan_can_atlas_2018.tar.gz',\"data_mrna_seq_v2_rsem.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4HT7jtTdZIv"
   },
   "source": [
    "We now merge the datasets, and see too that we only include transcripts that are measured in all the carcinomas with an count larger than 0. Further we scale the measurements so that every gene expression value is scaled between 0 and 1, using sk-learns [MinMaxScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTGtXhUgdZIw"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "combined = pd.concat([lusc[lusc.index.notna()] , luad[luad.index.notna()]], axis=1, sort=False)\n",
    "# Drop rows with any missing values\n",
    "combined.dropna(axis=0, how='any', inplace=True)\n",
    "combined = combined.loc[~(combined<=0.0).any(axis=1)]\n",
    "combined.index = combined.index.astype(str)\n",
    "X=scaler.fit_transform(np.log2(combined).T).T\n",
    "combined = pd.DataFrame(data=X,index=combined.index,columns=combined.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgRZHdghdZIw"
   },
   "source": [
    "We are setting up an istance of a machine learning framework, [PyTorch](https://en.wikipedia.org/wiki/PyTorch). It will help us fitting the needed neural network. We also define a dataloader, that will help us load the data for processing in the tensorlibrary, torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIY4kACMdZIx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Setting training parameters\n",
    "batch_size, lr, epochs, log_interval = 256, 1e-3, 2501, 500\n",
    "hidden_dim, latent_dim = 2048, 12\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(4711)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "# Convert combined DataFrame to a PyTorch tensor\n",
    "datapoints = torch.tensor(combined.to_numpy().T, dtype=torch.float32)\n",
    "labels = torch.tensor([1.0 for _ in lusc.columns] + [0.0 for _ in luad.columns], dtype=torch.float32)\n",
    "\n",
    "# Use TensorDataset to create a dataset\n",
    "dataset = TensorDataset(datapoints, labels)\n",
    "\n",
    "# DataLoader for batching the data\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we design the VAE. We use an architecure where 13046 features are first throtteled down to a lower number of hidden features (fc1) then to 12 features, which we predict both mean and variance for (fc21 and fc22).   \n",
    "\n",
    "We reparametrize those 12 variables, and then expand them to a larger number hiden nodes (fc3) and 13046 (fc4) features.\n",
    "![](img/nn.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)       # Input layer of encoder\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim) # Output layer encoder (mean)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim) # Output layer encoder (stdv)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)  # Input layer of decoder\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)       # Output layer of decoder\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)         # Dividing a logged value by two results in the log of the sqrt of the value \n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        out = torch.sigmoid(self.fc4(h3))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we next select a gradient descent optimizer, [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam), and select a fuction to optimize, the loss_function, and we define a train and test procedure to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = combined.shape[0]\n",
    "model = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # Calculating the Kullbackâ€“Leibler divergence\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    #print(f\"BCE={BCE}, KLD={KLD}\")\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch % log_interval == 0:\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "#        for i, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "    if epoch % log_interval == 0:\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are set to run the procedure for 2000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we for this dataset have chosen to not use an independent testset. We have now trained our VAE. We can first evaluate it for the datapoints we trained it on, and get their embeddings in a vector, $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "x_batch, z_batch, std_batch = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x, _) in enumerate(test_loader):\n",
    "        x = x.to(device)\n",
    "\n",
    "        x_hat_, mean_, log_var = model(x)\n",
    "        x_batch.append(x_hat_.cpu().detach().numpy())\n",
    "        z_batch.append(mean_.cpu().detach().numpy())\n",
    "        std_batch.append(torch.exp(log_var * 0.5).cpu().detach().numpy())\n",
    "\n",
    "x_hat = np.concatenate(x_batch, axis=0)\n",
    "z = np.concatenate(z_batch, axis=0)\n",
    "std = np.concatenate(std_batch, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeKF0wIYdZIx"
   },
   "source": [
    "We can now use the embeddings to describe our data. We first plot the differeces between the different latent variables for the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Create DataFrame for transformed patients\n",
    "transformed_patients = pd.DataFrame(\n",
    "    data=z,\n",
    "    columns=[f\"Latent Variable {ix+1}\" for ix in range(latent_dim)],\n",
    "    index=list(lusc.columns) + list(luad.columns)\n",
    ")\n",
    "transformed_patients[\"Set\"] = [\"LUSC\" for _ in lusc.columns] + [\"LUAD\" for _ in luad.columns]\n",
    "transformed_patients[\"Label\"] = [1 for _ in lusc.columns] + [0 for _ in luad.columns]  # Assign numerical labels for AUC calculation\n",
    "\n",
    "# Set Seaborn plotting style\n",
    "sns.set(style=\"white\", rc={\"axes.titlesize\": 14, \"axes.labelsize\": 12})\n",
    "\n",
    "# Determine grid size\n",
    "plots_per_row = 4\n",
    "n_rows = int(np.ceil(latent_dim / plots_per_row))\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, plots_per_row, figsize=(20, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each latent variable's histogram and calculate AUC\n",
    "for i in range(latent_dim):\n",
    "    latent_variable_name = f\"Latent Variable {i+1}\"\n",
    "    \n",
    "    # Calculate AUC for the current latent variable\n",
    "    auc = roc_auc_score(transformed_patients[\"Label\"], transformed_patients[latent_variable_name])\n",
    "    auc = max(auc, 1-auc)\n",
    "    \n",
    "    # Plot histogram\n",
    "    sns.histplot(\n",
    "        data=transformed_patients,\n",
    "        x=latent_variable_name,\n",
    "        hue=\"Set\",\n",
    "        kde=True,\n",
    "        element=\"step\",\n",
    "        bins=30,\n",
    "        palette=\"Set1\",\n",
    "        ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f\"Distribution of {latent_variable_name}\\nAUC = {auc:.2f}\")\n",
    "    axes[i].set_xlabel(latent_variable_name)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Remove any empty subplots if latent_dim is not divisible by plots_per_row\n",
    "for j in range(latent_dim, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that variables 5 and 10 seem to be the most discriminating latent variables between the sets.\n",
    "Much like for the PCA we can use the embeddings to give a dimentionallity reduced description of each cancer's expression profile using those two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvnYbO2ZdZIy",
    "outputId": "fe7bfe90-fc1c-4b22-ce28-82887e6d1673"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "transformed_patients = pd.DataFrame(data=z,columns=[f\"Latent Variable {ix+1}\" for ix in range(latent_dim)],index=list(lusc.columns) + list(luad.columns))\n",
    "transformed_patients[\"Set\"]= ([\"LUSC\" for _ in lusc.columns]+[\"LUAD\" for _ in luad.columns])\n",
    "\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "sns.set_style(\"white\")\n",
    "#sns.set_context(\"talk\")\n",
    "\n",
    "lm = sns.lmplot(x=\"Latent Variable 5\",y=\"Latent Variable 10\", hue='Set', data=transformed_patients, fit_reg=False)\n",
    "#for x, y, (w, h) in zip(transformed_patients[\"Latent Variable 1\"], transformed_patients[\"Latent Variable 2\"], std):\n",
    "#    lm.axes[0, 0].add_patch(Ellipse((x,y), w, h, fc='#CCCCCC', lw=1, alpha=0.5, zorder=1))\n",
    "means={}\n",
    "for name,set_ in transformed_patients.groupby(\"Set\"):\n",
    "    means[name] = set_.mean(numeric_only=True).to_numpy()\n",
    "    plt.scatter(means[name][5-1],means[name][10-1], marker='^',s=30,c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6WhJwTDdZIz"
   },
   "source": [
    "Here we see a quite good, but not perfect separation of the patients based on two latent variables.\n",
    "\n",
    "## Using the Decoder for generating example data\n",
    "\n",
    "Furter, we can use the network to generate \"typical\" expression profiles. We have marked the means of each sample with black triangles. We will now take these mean values of each patient group and use them as representation of each cancer type, and feed these two values for each patient group to the VAE's decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVFzjS1bdZI0",
    "outputId": "931c26d8-c578-4a9c-c5de-2c139cdf10d1"
   },
   "outputs": [],
   "source": [
    "z_fix = torch.tensor(np.concatenate(([means[\"LUSC\"]],[means[\"LUAD\"]]), axis=0))\n",
    "\n",
    "z_fix = z_fix.to(device)\n",
    "x_fix = model.decode(z_fix).cpu().detach().numpy()\n",
    "predicted = pd.DataFrame(data=x_fix.T, index=combined.index, columns=[\"LUSC\", \"LUAD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JxtZLn6dZI1"
   },
   "source": [
    "Using these generated profiles we may for instance identify the genes most differential between the generated LUSC and LUAD sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdGnIpvgdZI1",
    "outputId": "3fcf9449-bfea-443c-d2fc-179054e1c906"
   },
   "outputs": [],
   "source": [
    "predicted[\"diff\"] = predicted[\"LUSC\"] - predicted[\"LUAD\"]\n",
    "# predicted.sort_values(by='diff', ascending=False, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKnwZ3DUdZI1"
   },
   "source": [
    "The genes that the dencoder find most different between the set means  can now be identified. First the gene lwith largest difference between the LUSC and LUAD in positive direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORn2eerZdZI2",
    "outputId": "34a12ca0-4017-4d0f-afde-b5bca0b22e50"
   },
   "outputs": [],
   "source": [
    "predicted[\"diff\"].idxmin(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a [cancer related](https://www.proteinatlas.org/ENSG00000172731-LRRC20/cancer) protein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then in negative direction (larger in LUAD than LUSC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted[\"diff\"].idxmax(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a [prognostic marker](https://www.proteinatlas.org/ENSG00000146054-TRIM7/cancer) for survival in LUAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-OJDYaFdZI2"
   },
   "source": [
    "Here these two genes seems to be the largest differentiators beteen the genes in LUSC and LUAD. We can also note that as with PCA, the Gene KRT17 seems quite different between the cancer types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZ4Mo7rVdZI2"
   },
   "outputs": [],
   "source": [
    "predicted.loc[\"KRT17\"]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "https://github.com/statisticalbiotechnology/cb2030/blob/master/nb/pca/PCAofCarcinomas.ipynb",
     "timestamp": 1666187194313
    }
   ]
  },
  "kernelspec": {
   "display_name": "jb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
